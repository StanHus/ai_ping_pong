# AI Ping-Pong Studio â€“ **Master Reference Guide**

_Last updated: 2025-06-17_

---

## Table of Contents

1. [Introduction](#introduction)
2. [Philosophy & Background](#philosophy--background)
3. [End-to-End Workflow](#end-to-end-workflow)
4. [Tech Stack](#tech-stack)
5. [High-Level Architecture](#high-level-architecture)
6. [Folder & File Structure](#folder--file-structure)
7. [Frontend](#frontend)
8. [Backend / API Layer](#backend--api-layer)
9. [Prompt Library](#prompt-library)
10. [Default Scenarios & Workflows](#default-scenarios--workflows)
11. [Testing Strategy](#testing-strategy)
12. [Deployment Guide](#deployment-guide)
13. [Roadmap & Future Work](#roadmap--future-work)
14. [References](#references)

---

## Introduction

**AI Ping-Pong Studio** is a minimal but powerful multi-LLM orchestration playground.  
Instead of automated chains, the system **bounces** content between specialist language modelsâ€”_GPT, Grok, Claude, Gemini_â€”leveraging each model's comparative advantage while keeping a human-in-the-loop.

> "Like table-tennis, each volley sharpens the final result."

The project currently ships as a Vite + React front-end with a thin, model-agnostic API wrapper. All state is local to the browserâ€”no database required.

---

## Philosophy & Background

- **Interpretability over automation** â€“ manual checkpoints keep humans accountable and able to intervene.
- **Capability-aware routing** â€“ each model is assigned work that suits its strengths (e.g. Grok â†’ live research, Claude â†’ structure).
- **Public, subscription-free tooling** â€“ everything works with the free web UIs or public APIs.
- **Rapid iteration** â€“ short, high-leverage loops (5-13 steps) take ~10-15 minutes per assignment.

The idea contrasts with heavy agent-frameworks (LangChain, AutoGPT, etc.) by stressing **_clarity, flexibility and zero-setup_**.

---

## End-to-End Workflow

```
GPT âžœ Grok âžœ GPT âžœ Claude âžœ GPT  (Default 5-Step)
        â†˜ï¸Ž optional loops â†™ï¸Ž
```

1. **GPT** â€“ Task definition & orchestration
2. **Grok** â€“ Deep, real-time research
3. **GPT** â€“ Integrate research, draft Claude brief
4. **Claude** â€“ Logical structure & outline
5. **GPT** â€“ Final editorial polish

Enhanced flows add Gemini, extra validation checkpoints and repetition (up to 13 steps).

---

## Tech Stack

| Layer       | Choice                                                          | Notes                                                  |
| ----------- | --------------------------------------------------------------- | ------------------------------------------------------ |
| Front-end   | React 18 + Vite + TailwindCSS                                   | Instant HMR & bare-bones styling                       |
| State       | Zustand                                                         | Simpler than Redux, good for local storage persistence |
| Backend     | Lightweight _Next.js API routes_ **or** single _Express_ server | Swappableâ€”only one `POST /api/run` route needed        |
| Model SDKs  | `openai`, `anthropic`, REST wrapper for Grok/Firecrawl          | Each wrapped in `src/lib`                              |
| Persistence | `localStorage`                                                  | No DB â€“ portable PWA style                             |
| Tooling     | Vitest + React Testing Library                                  | See **Testing Strategy**                               |

---

## High-Level Architecture

### 1. Driver Architecture

A unified interface normalises every model call to:  
`Promise<ReadableStream<LLMChunk>>`

```typescript
interface LLMDriver {
  name: LLM;
  call(prompt: string, context: string[]): Promise<ReadableStream>;
  limit: number; // context tokens
  caps: string[]; // capability tags e.g. "live-web"
}
```

### 2. Workflow Engine

Keeps track of ordered `WorkflowStep[]`, context and fallbacks.

```typescript
runStep(step: WorkflowStep, ctx: WorkflowContext): Promise<ReadableStream>
```

_Capability-aware routing_ picks the best driver for each step, with JSON-defined fallback chains.

### 3. Token & Stream Management

All driver streams are wrapped to emit a standard JSON chunk protocol:

```json
{ "type": "token", "driver": "gpt", "content": "..." }
```

Smart truncation utilities keep prompts inside each model's context window (see `docs/ARCHITECTURE.md`).

---

## Folder & File Structure

```
â”œâ”€ src/
â”‚  â”œâ”€ components/
â”‚  â”‚  â”œâ”€ StepCard.tsx      # prompt card UI & run button
â”‚  â”‚  â”œâ”€ Timeline.tsx      # vertical list wrapper
â”‚  â”‚  â””â”€ Guide.tsx         # educational guide + copy-paste prompts
â”‚  â”œâ”€ lib/
â”‚  â”‚  â”œâ”€ openai.ts         # GPT wrapper
â”‚  â”‚  â”œâ”€ claude.ts         # Claude wrapper
â”‚  â”‚  â”œâ”€ grok.ts           # Grok/Firecrawl wrapper (mock fallback)
â”‚  â”‚  â”œâ”€ enhanced-prompts.ts # 11- & 13-step workflows
â”‚  â”‚  â””â”€ workflow-engine.ts   # capability routing & context mgmt
â”‚  â”œâ”€ store/
â”‚  â”‚  â””â”€ useSession.ts     # Zustand global store
â”‚  â”œâ”€ App.tsx              # Layout & router
â”‚  â””â”€ main.tsx             # Vite entrypoint
â”œâ”€ prompts/                # Plain-text prompt library
â”‚  â”œâ”€ gpt.txt
â”‚  â”œâ”€ grok.txt
â”‚  â””â”€ claude.txt
â””â”€ docs/                   # Additional in-depth guides (testing, deploymentâ€¦)
```

---

## Frontend

### Components

- **StepCard** â€“ Reusable UI block that shows the prompt, lets the user _copy_ or _run_ it, and reveals streaming output.
- **Timeline** â€“ Simply maps over an array of steps and renders `StepCard` in order, disabling the _Run_ button until the prior step is complete.
- **Guide** â€“ The in-product markdown guide (see `src/components/Guide.tsx`). It hosts:
  - _What / Why / Tips_ panels
  - Pre-made 5-step workflow
  - Ultra-Enhanced 11 & 13-step workflows
  - One-click _Copy_ buttons for each prompt

### State Management (`store/useSession.ts`)

```typescript
interface SessionState {
  steps: WorkflowStep[];
  outputs: (string | null)[];
  runStep: (i: number) => Promise<void>;
}
```

Persisted in `localStorage` for browser reload safety.

---

## Backend / API Layer

Single route â†’ **POST `/api/run`**

```ts
switch (body.model) {
  case "gpt":
    return openAI(body.prompt, body.context);
  case "claude":
    return anthropic(body.prompt, body.context);
  case "grok":
    return grokSearch(body.prompt);
}
```

If an API key is missing the wrapper returns a _mock_ stream so the UI still functions offline.

---

## Prompt Library

Below are the canonical prompts embedded in `Guide.tsx`. Feel free to customise per assignment.

### 1. GPT â€” _Mediator, Evaluator & Editorial Orchestrator_

```text
You are the initiator and coordinator of the "AI Ping-Pong" workflow. You will:
	â€¢	Start by synthesizing the initial idea into a complete task definition, requirements list, and evaluation plan.
	â€¢	Send detailed research prompts to Grok
	â€¢	Interpret Grok's findings
	â€¢	Structure instructions for Claude
	â€¢	Evaluate Claude's structure
	â€¢	Loop back to Grok (if additional gaps are found)
	â€¢	Conduct the final editorial and strategic polish before publication

ðŸ§­ GPT Master Prompt: Mediator, Evaluator, and Editorial Orchestrator

You are the central orchestrator in a structured, manual multi-model workflow called AI Ping-Pong, which cycles content through Claude (structure and reasoning), Grok (deep research), and GPT (you) to iteratively refine and elevate output.
Your job is to sit at the center of this process and act as:
	â€¢	The evaluator of ideas and content
	â€¢	The editor that improves clarity, logic, tone, and structure
	â€¢	The mediator that guides next steps, detects gaps, and integrates inputs across the chain

You will be handed:
	â€¢	A Statement of Work that outlines the full intent, deliverables, tone, quality requirements, structure, and strategic goals of the project
	â€¢	A structured draft or content blocks produced by Claude
	â€¢	Research summaries and citations produced by Grok

â¸»

Your responsibilities as GPT:

1. Evaluate Claude's structured output:
	â€¢	Does the article follow the requested structure? (Intro â†’ Methodology â†’ Validation â†’ Strengths â†’ Use Cases â†’ Infographic â†’ Conclusion)
	â€¢	Are all sections clear, logical, and compelling?
	â€¢	Are there redundancies, weak transitions, or confusing phrasings?
	â€¢	Highlight any unclear logic or assumptions.

2. Integrate Grok's research precisely:
	â€¢	Ensure that all claims or historical references in the draft are backed by real, verifiable sources from Grok's output.
	â€¢	Check that citations are embedded clearly and match the claims.
	â€¢	Suggest places where deeper sourcing or evidence would strengthen the argument.

3. Edit for tone, structure, and clarity:
	â€¢	Match the tone and clarity of the sample article "Validated 10-Minute AI-to-Slides Workflow."
	â€¢	Ensure the voice is confident, direct, slightly opinionated, and efficient.
	â€¢	Use sharp transitions and plain language wherever possible.
	â€¢	Remove jargon or vagueness.

4. Mediate and propose next steps:
	â€¢	Identify where Claude or Grok may need to rework or expand content.
	â€¢	Suggest what parts of the workflow need a second pass â€” structure, research, logic, framing, tone, etc.
	â€¢	Optionally, generate short next-step prompts to send back to Claude or Grok to address gaps.

5. Final Quality Control:
	â€¢	Confirm that the article, once polished, will meet the quality bar for Trilogy's CoE Microsite and Substack.
	â€¢	Ensure it aligns with the CoE's goals: fast-cycle publishing, validated insight, strong POVs, and strategic internal relevance.
	â€¢	If it doesn't meet the standard, clearly identify what's missing and how to resolve it.

â¸»

Output Format:
	â€¢	Use labeled sections (e.g., âœ¦ Evaluation, âœ¦ Integration Check, âœ¦ Editorial Notes, âœ¦ Next Steps, âœ¦ Final QC Summary)
	â€¢	Use bullet points where helpful
	â€¢	Be decisive and editorial â€” you are not just a summarizer, but a lead editor across the AI workflow

â¸»

Once this role is defined, you will be responsible for the final pass on any AI Ping-Pong output before publication. Be thorough, critical, and constructive.
```

### 2. Grok â€” _Deep Research & Citations_

```text
You are acting as a research assistant helping to validate and contextualize a new manual AI workflow called AI Ping-Pong, which systematically cycles content through a series of public LLMs â€” typically:
Claude â†’ GPT â†’ Grok â†’ GPT â†’ Claude

The idea is to leverage each model's unique strengths in a structured, manual (non-automated) iterative cycle. For example:
	â€¢	Claude is used for deep reasoning and content structuring.
	â€¢	GPT is used as the central mediator â€” interpreting structure, planning next steps, checking alignment, and refining output.
	â€¢	Grok is used for deep, timely research to enrich and fact-check the content.
	â€¢	Then the process cycles again: GPT interprets Grok's research, Claude restructures the output, and so on.

Your task is to conduct an exhaustive, comprehensive search into whether this concept has already been explored â€” and if so, how, when, why, and with what results.
This workflow will be the foundation for a technical article and infographic intended to influence high-level AI strategy at Trilogy and across multiple departments.

I want your research output to be highly structured, citation-backed, and explicitly answer the following:
	1.	Has anything like this been done before?
	â€¢	Look for any blog posts, whitepapers, GitHub repos, case studies, or published research that describes multi-LLM workflows where each model has a defined role and the process is iterative and manual (i.e., not one-click or API automation).
	â€¢	If similar workflows exist (e.g., in academia, product teams, tool comparisons, AI orchestration), describe them in detail.
	2.	How does this compare to automated orchestration tools like n8n, LangChain, or AutoGPT?
	â€¢	List pros and cons of manual vs. automated chaining.
	â€¢	Emphasize interpretability, flexibility, feature adoption, speed, and error handling.
	â€¢	Cite expert perspectives or case studies where possible.
	3.	Why haven't we seen more of this in industry â€” or why hasn't it caught on yet?
	â€¢	Is it a UX problem? Trust? Lack of awareness? Friction?
	â€¢	Provide informed, cited speculation or opinionated writing from credible sources.
	4.	Who (if anyone) is talking about this idea?
	â€¢	Find and quote respected thought leaders who've commented on multi-agent AI workflows or manual+human-in-the-loop systems.
	â€¢	Prioritize industry leaders like Marc Benioff (Salesforce), Rob Thomas (IBM), Jason Fried (Basecamp), or those writing at Anthropic, OpenAI, Google DeepMind, etc.
	5.	Include references for every major claim.
	â€¢	Prioritize source links to public, verifiable resources: blog posts, interviews, peer-reviewed papers, GitHub repos, case studies, or reputable industry articles.
	â€¢	Include author names, publication dates, and exact source titles where possible.

Output format:
	â€¢	Organize findings into clearly labeled sections.
	â€¢	Bullet-point all key facts.
	â€¢	Include source links at the end of each bullet.
	â€¢	Highlight contradictions or differing opinions where relevant.
```

### 3. Claude â€” _Logical Architect & Structure Guru_

```text
You are acting as a senior technical writer structuring a high-clarity, high-impact article aimed at AI researchers, engineers, and product leaders across Trilogy.

The piece will describe a novel, structured, manual workflow called AI Ping-Pong, which cycles content between multiple LLMs in the following order:
GPT â†’ Grok â†’ GPT â†’ Claude â†’ GPT â†’ Grok (if needed) â†’ GPT â€” to iteratively refine ideas, gather validated research, and produce structured execution-ready content.

This method leverages each model's unique strengths:
	â€¢	GPT as the primary orchestrator, mediator, and content integrator
	â€¢	Grok for deep research and verified sourcing
	â€¢	Claude for clean logical structure, reasoning, and outlining

This is a manual, human-driven alternative to automated AI orchestration tools like n8n, LangChain, or AutoGPT. The focus is on interpretability, adaptability, and maximizing public model capabilities without code or rigid sequences.

You are brought into the process after:
	â€¢	GPT has synthesized the initial Statement of Work
	â€¢	Grok has returned the first round of validated research
	â€¢	GPT has evaluated both and defined the structure goals

You will use:
	â€¢	A detailed Statement of Work outlining scope, deliverables, and success criteria
	â€¢	Verified research findings from Grok, including expert validation and prior art
	â€¢	A tone and formatting reference modeled on: "Validated 10-Minute AI-to-Slides Workflow"
	â€¢	Your role is to translate this input into a clearly structured, block-by-block article draft, to be reviewed and refined by GPT before publication.
â¸»

Your task is to structure the full article in detail â€” block-by-block â€” using the following outline:

1. Introduction
	â€¢	Set the context: the AI tool landscape, the rise of automation, the gaps in current workflows.
	â€¢	Introduce the concept of AI Ping-Pong and what makes it novel.
	â€¢	Hook the reader with a statement like: "Why it matters now."

2. Purpose & Methodology
	â€¢	Explain the rationale for the project.
	â€¢	Describe each step of the workflow with full clarity (e.g., what Claude does, how GPT interprets, what Grok delivers).
	â€¢	Use clear transitions to show how content moves between models.
	â€¢	Include prompt examples or templates if useful.

3. Validation & Historical Context
	â€¢	Summarize Grok's research on whether similar ideas have been tried before.
	â€¢	Clearly show how this approach builds on, diverges from, or improves upon past work.
	â€¢	Cite 3â€“5 sources or expert validations with clear links.

4. Strengths & Limitations
	â€¢	Bullet-point each core advantage of this workflow (e.g., flexibility, speed, interpretability, instant access to latest features).
	â€¢	Include 1â€“2 real limitations or challenges (e.g., manual overhead, reliance on user skill).
	â€¢	Back each claim with a source or rationale.

5. Use Cases & Departmental Fit
	â€¢	Provide 2â€“3 specific, realistic applications of this workflow inside Trilogy: e.g., content ops, research, strategy, marketing.
	â€¢	Make it obvious why this method applies beyond engineers.

6. Infographic Blueprint
	â€¢	Write a paragraph clearly describing what the infographic should include (visual workflow, role of each model, looping structure, mediation layer, benefits).
	â€¢	Make this so clear a designer could execute it with no back-and-forth.

7. Conclusion & Strategic Alignment
	â€¢	Tie the value of this workflow to Trilogy's CoE goals.
	â€¢	Reinforce its relevance to fast-cycle publishing, public innovation, and internal tooling efficiency.
	â€¢	End with a strong version of: "Why this matters now."

â¸»

Final Instructions:
	â€¢	Do not write the prose yet â€” structure everything in labeled sections with full clarity.
	â€¢	Language should mirror the tone of the sample article: sharp, confident, efficient, and opinionated.
	â€¢	Use Grok's research wherever citations are required.
	â€¢	Mark any missing pieces that I need to fill in before finalizing.
```

---

## Default Scenarios & Workflows

### 5-Step "Article" Flow (Guide.tsx)

| #   | Model  | Title                                |
| --- | ------ | ------------------------------------ |
| 1   | GPT    | Task Definition & Plan               |
| 2   | Grok   | Deep Research & Citations            |
| 3   | GPT    | Evaluate & Draft Claude Instructions |
| 4   | Claude | Structure the Article / Asset        |
| 5   | GPT    | Final Editorial Polish               |

> _Tip:_ Only unlock the next step after the previous one is **completed & reviewed**.

### Ultra-Enhanced Article Workflow (13 Steps)

Key steps pulled from `src/lib/enhanced-prompts.ts`:

1. Gemini â€“ Deep Content Understanding
2. GPT â€“ Strategic Task Clarification
3. Grok â€“ Comprehensive Real-Time Research
4. Gemini â€“ Source Validation
5. Claude â€“ Advanced Logical Architecture
6. GPT â€“ Premium Content Draft  
   (_â€¦plus 7 further polish and validation stepsâ€”see code for full list._)

### Ultra-Enhanced Research Workflow (11 Steps)

Follows a similar structure with heavier emphasis on data verification.

---

## Testing Strategy

High-level test suites stored in `docs/TESTING.md` focus on:

- **Unit** â€“ driver wrappers, truncation util, store actions.
- **Integration** â€“ `/api/run` happy-path & error fallbacks.
- **E2E** â€“ Cypress script that runs the 5-step flow with mocked API responses.

---

## Deployment Guide

1. **Build**: `pnpm run build` â€“ outputs `dist/` for static hosting.
2. **Preview**: `pnpm run preview` â€“ local prod server.
3. **Edge Functions**: API route is serverless-friendly; deploy to Vercel/Netlify in <30 s.  
   Full details in `docs/DEPLOYMENT.md`.

---

## Roadmap & Future Work

- [ ] Live SSE streaming for token-level updates.
- [ ] OAuth login; save sessions to user profiles.
- [ ] Real Grok DeepSearch once API is stable.
- [ ] One-click _Export â†’ Markdown_ using `remark`.
- [ ] Interactive _prompt editor_ with version history.

---

## References

- Original one-shot repo scaffold: `task/plan.txt`
- In-depth architecture diagrams: `docs/ARCHITECTURE.md`
- Testing matrix: `docs/TESTING.md`
- Deployment cookbook: `docs/DEPLOYMENT.md`
- API contract: `docs/API.md`

---

> Â© 2024 â€“ AI Ping-Pong Studio  
> MIT License
